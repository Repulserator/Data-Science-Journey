{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import feedparser\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data (Part 1)\n",
    "\n",
    "Ideally you want to start by collapsing all the parts (1 of 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Scouting the basic format of rss feed,\\\n",
    "I first scraped all the rss links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RSS_FEEDS = [\n",
    "    \"http://rss.cnn.com/rss/cnn_topstories.rss\",\n",
    "    \"http://qz.com/feed\",\n",
    "    \"http://feeds.foxnews.com/foxnews/politics\",\n",
    "    \"http://feeds.reuters.com/reuters/businessNews\",\n",
    "    \"http://feeds.feedburner.com/NewshourWorld\",\n",
    "    \"https://feeds.bbci.co.uk/news/world/asia/india/rss.xml\"\n",
    "]\n",
    "\n",
    "def parse_rss_feeds() -> pd.DataFrame:\n",
    "    articles_data = []\n",
    "\n",
    "    for feed_url in RSS_FEEDS:\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        for entry in feed.entries:\n",
    "            title = entry.title if hasattr(entry, 'title') else 'NA'\n",
    "            content = entry.summary if hasattr(entry, 'summary') else (entry.title_detail.value if hasattr(entry, 'title_detail') else 'NA')\n",
    "            published = entry.published if hasattr(entry, 'published') else 'NA',\n",
    "            url = entry.link if hasattr(entry, 'link') else 'NA'\n",
    "            source = feed.feed.title if hasattr(feed.feed, 'title') else 'NA'\n",
    "\n",
    "            article_data = {\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"published\": published,\n",
    "                \"url\":url,\n",
    "                \"source\": source\n",
    "            }\n",
    "            articles_data.append(article_data)\n",
    "\n",
    "    df = pd.DataFrame(articles_data)\n",
    "    return df\n",
    "\n",
    "articles_df = parse_rss_feeds()\n",
    "print(articles_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After The links next was to scrape the body and headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_headers_body(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            headers = [header.text.strip() for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "            body_text = [text.text.strip() for text in soup.find_all('p')]\n",
    "            \n",
    "            return headers, body_text\n",
    "        else:\n",
    "            print(f\"Error: Unable to retrieve content from the URL: {url}\")\n",
    "            return [], []\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping URL: {url}. Error: {str(e)}\")\n",
    "        return [], []\n",
    "\n",
    "articles_df[['headers', 'body_text']] = articles_df['url'].apply(lambda x: pd.Series(scrape_headers_body(x)))\n",
    "\n",
    "print(articles_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will save the Failed links here to revisit later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(columns=articles_df.columns)\n",
    "\n",
    "articles_df['headers'] = articles_df['headers'].apply(lambda x: None if len(x) < 3 else x)\n",
    "articles_df['body_text'] = articles_df['body_text'].apply(lambda x: None if len(x) < 3 else x)\n",
    "articles_df['published'] = articles_df['published'].apply(lambda x: None if x == \"('NA',)\" else x)\n",
    "\n",
    "failed_df = failed_df.append(articles_df[(articles_df['headers'].isna()) | (articles_df['body_text'].isna())])\n",
    "\n",
    "articles_df = articles_df.dropna(subset=['headers', 'body_text'])\n",
    "\n",
    "print(\"Modified Articles DataFrame:\")\n",
    "print(articles_df)\n",
    "print(\"\\nFailed URLs DataFrame:\")\n",
    "print(failed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database (PART 2)\n",
    "\n",
    "The database should be explained in the readme file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### Preprocess\n",
    "Before ingesting it to the database we must preprocess it \\\n",
    "Since, We have articles, I took the liberty of loading it from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(\"Hey2c.csv\")\n",
    "articles_df.set_index('Unnamed: 0', inplace=True)\n",
    "articles_df.index.name = 'index'\n",
    "articles_df = articles_df.drop(index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On a broad level:\n",
    "# Fix body_text that was scraped as a list and fixed time which is the most important part when we put it in  a database\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        words_to_remove = ['Read more', 'Click here', 'This video can not be played', 'CNN values your feedback', 'Markets', 'Hot Stocks', 'Fear & Greed Index','CNN values your feedback','Markets', 'Hot Stocks', 'Fear & Greed Index', 'Latest Market News', 'Hot Stocks']\n",
    "        text = re.sub(r'|'.join(map(re.escape, words_to_remove)), '', text,flags=re.IGNORECASE)\n",
    "        return text.strip()\n",
    "    \n",
    "    elif isinstance(text, list):\n",
    "        processed_text = [preprocess_text(sub_text) for sub_text in text if sub_text.strip()]\n",
    "        return processed_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def preprocess_date(date_string):\n",
    "\n",
    "    if 'GMT' not in date_string and 'UTC' not in date_string:\n",
    "        time_zone_offset_str = date_string[-5:]\n",
    "        date_obj = datetime.strptime(date_string[:-6], '%a, %d %b %Y %H:%M:%S')\n",
    "\n",
    "        time_zone_offset = timedelta(\n",
    "            hours=int(time_zone_offset_str[1:3]),\n",
    "            minutes=int(time_zone_offset_str[3:])\n",
    "        )\n",
    "\n",
    "        if time_zone_offset_str[0] == '+':\n",
    "            date_obj = date_obj - time_zone_offset\n",
    "        elif time_zone_offset_str[0] == '-':\n",
    "            date_obj = date_obj + time_zone_offset\n",
    "\n",
    "        date_utc_str = date_obj.strftime('%Y-%m-%d %H:%M:%S+00:00')\n",
    "\n",
    "        return date_utc_str\n",
    "    else:\n",
    "        if 'GMT' in date_string:\n",
    "            date_string = date_string.replace('GMT', '+0000')\n",
    "        elif 'IST' in date_string:\n",
    "            date_string = date_string.replace('IST', '+0530') \n",
    "\n",
    "        return preprocess_date(date_string)\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \n",
    "    df = df.applymap(lambda x: None if str(x) in ('NA',\"[]\") else x)\n",
    "\n",
    "    try:\n",
    "        df['published'] = df['published'].apply(lambda x: None if x is None or len(str(x)) < 8 else x)\n",
    "        df.dropna(inplace=True)\n",
    "        df['published'] = df['published'].str[2:-3].apply(preprocess_date)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred in fixing date format: {e}\")\n",
    "\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df[\"body_text\"] = df[\"body_text\"].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "df = articles_df.copy()\n",
    "\n",
    "# Preprocess dataframe\n",
    "df = preprocess_dataframe(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>published</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>headers</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some on-air claims about Dominion Voting Syste...</td>\n",
       "      <td>Some on-air claims about Dominion Voting Syste...</td>\n",
       "      <td>2023-04-19 12:44:51+00:00</td>\n",
       "      <td>https://www.cnn.com/business/live-news/fox-new...</td>\n",
       "      <td>CNN.com - RSS Channel - HP Hero</td>\n",
       "      <td>['CNN values your feedback', 'Settlement reach...</td>\n",
       "      <td>our live coverage has ended follow the latest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here are the 20 specific Fox broadcasts and tw...</td>\n",
       "      <td>• Fox-Dominion trial delay 'is not unusual,' j...</td>\n",
       "      <td>2023-04-17 16:01:11+00:00</td>\n",
       "      <td>https://www.cnn.com/2023/04/17/media/dominion-...</td>\n",
       "      <td>CNN.com - RSS Channel - HP Hero</td>\n",
       "      <td>['CNN values your feedback', 'Here are the 20 ...</td>\n",
       "      <td>fear  greed index   for all the interest in bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Judge in Fox News-Dominion defamation trial: '...</td>\n",
       "      <td>The judge just announced in court that a settl...</td>\n",
       "      <td>2023-04-19 08:28:17+00:00</td>\n",
       "      <td>https://www.cnn.com/2023/04/18/media/fox-domin...</td>\n",
       "      <td>CNN.com - RSS Channel - HP Hero</td>\n",
       "      <td>['CNN values your feedback', 'Fox News settles...</td>\n",
       "      <td>fear  greed index   fox news reached a lastsec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Difficult to say with a straight face': Tappe...</td>\n",
       "      <td>A settlement has been reached in Dominion Voti...</td>\n",
       "      <td>2023-04-18 21:17:44+00:00</td>\n",
       "      <td>https://www.cnn.com/videos/politics/2023/04/18...</td>\n",
       "      <td>CNN.com - RSS Channel - HP Hero</td>\n",
       "      <td>['CNN values your feedback', '‘Difficult to sa...</td>\n",
       "      <td>cable news network a warner bros discovery com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Millions in the US could face massive conseque...</td>\n",
       "      <td>• DeSantis goes to Washington, a place he once...</td>\n",
       "      <td>2023-04-18 20:34:45+00:00</td>\n",
       "      <td>https://www.cnn.com/2023/04/18/politics/mccart...</td>\n",
       "      <td>CNN.com - RSS Channel - HP Hero</td>\n",
       "      <td>['CNN values your feedback', 'The US economy c...</td>\n",
       "      <td>millions of americans could face massive conse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "index                                                      \n",
       "0      Some on-air claims about Dominion Voting Syste...   \n",
       "2      Here are the 20 specific Fox broadcasts and tw...   \n",
       "3      Judge in Fox News-Dominion defamation trial: '...   \n",
       "4      'Difficult to say with a straight face': Tappe...   \n",
       "5      Millions in the US could face massive conseque...   \n",
       "\n",
       "                                                 content  \\\n",
       "index                                                      \n",
       "0      Some on-air claims about Dominion Voting Syste...   \n",
       "2      • Fox-Dominion trial delay 'is not unusual,' j...   \n",
       "3      The judge just announced in court that a settl...   \n",
       "4      A settlement has been reached in Dominion Voti...   \n",
       "5      • DeSantis goes to Washington, a place he once...   \n",
       "\n",
       "                       published  \\\n",
       "index                              \n",
       "0      2023-04-19 12:44:51+00:00   \n",
       "2      2023-04-17 16:01:11+00:00   \n",
       "3      2023-04-19 08:28:17+00:00   \n",
       "4      2023-04-18 21:17:44+00:00   \n",
       "5      2023-04-18 20:34:45+00:00   \n",
       "\n",
       "                                                     url  \\\n",
       "index                                                      \n",
       "0      https://www.cnn.com/business/live-news/fox-new...   \n",
       "2      https://www.cnn.com/2023/04/17/media/dominion-...   \n",
       "3      https://www.cnn.com/2023/04/18/media/fox-domin...   \n",
       "4      https://www.cnn.com/videos/politics/2023/04/18...   \n",
       "5      https://www.cnn.com/2023/04/18/politics/mccart...   \n",
       "\n",
       "                                source  \\\n",
       "index                                    \n",
       "0      CNN.com - RSS Channel - HP Hero   \n",
       "2      CNN.com - RSS Channel - HP Hero   \n",
       "3      CNN.com - RSS Channel - HP Hero   \n",
       "4      CNN.com - RSS Channel - HP Hero   \n",
       "5      CNN.com - RSS Channel - HP Hero   \n",
       "\n",
       "                                                 headers  \\\n",
       "index                                                      \n",
       "0      ['CNN values your feedback', 'Settlement reach...   \n",
       "2      ['CNN values your feedback', 'Here are the 20 ...   \n",
       "3      ['CNN values your feedback', 'Fox News settles...   \n",
       "4      ['CNN values your feedback', '‘Difficult to sa...   \n",
       "5      ['CNN values your feedback', 'The US economy c...   \n",
       "\n",
       "                                               body_text  \n",
       "index                                                     \n",
       "0      our live coverage has ended follow the latest ...  \n",
       "2      fear  greed index   for all the interest in bi...  \n",
       "3      fear  greed index   fox news reached a lastsec...  \n",
       "4      cable news network a warner bros discovery com...  \n",
       "5      millions of americans could face massive conse...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inserted the cleaned dataframe to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "db_string = 'postgresql://kaustubh:concept@localhost:4111/thetimeisnow'\n",
    "engine = create_engine(db_string)\n",
    "\n",
    "table_name = 'articles'\n",
    "\n",
    "df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Row inserted successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorization (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explored a few methods, but since i was on a deadline, i settled on using a huggingface pipeline after trying and testing\n",
    "- Originally i was going to use spacy text classification models, but this is faster\n",
    "- Also incorporated a small threshold to decide whether it really cant be classfied anywhere else but \"Others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n",
      "0.9999999478459358\n",
      "5\n",
      "0.19999998956918716\n",
      "0.20630082339048386 < 0.1\n",
      "============\n",
      "1.000000026077032\n",
      "5\n",
      "0.20000000521540642\n",
      "0.2063008077442646 < 0.1\n",
      "============\n",
      "0.999999949708581\n",
      "5\n",
      "0.1999999899417162\n",
      "0.20630082301795483 < 0.1\n",
      "============\n",
      "0.9999999552965164\n",
      "5\n",
      "0.19999999105930327\n",
      "0.20630082190036775 < 0.1\n",
      "============\n",
      "1.0000000186264515\n",
      "5\n",
      "0.2000000037252903\n",
      "0.2063008092343807 < 0.1\n",
      "============\n",
      "1.0000000223517418\n",
      "5\n",
      "0.20000000447034835\n",
      "0.20630080848932267 < 0.1\n",
      "============\n",
      "0.9999999552965164\n",
      "5\n",
      "0.19999999105930327\n",
      "0.20630082190036775 < 0.1\n",
      "============\n",
      "0.9999999776482582\n",
      "5\n",
      "0.19999999552965164\n",
      "0.20630081743001938 < 0.1\n",
      "============\n",
      "1.0\n",
      "5\n",
      "0.2\n",
      "0.206300812959671 < 0.1\n",
      "============\n",
      "1.0000000298023224\n",
      "5\n",
      "0.20000000596046447\n",
      "0.20630080699920655 < 0.1\n",
      "============\n",
      "0.9999999776482582\n",
      "5\n",
      "0.19999999552965164\n",
      "0.20630081743001938 < 0.1\n",
      "============\n",
      "0.9999999552965164\n",
      "5\n",
      "0.19999999105930327\n",
      "0.20630082190036775 < 0.1\n",
      "============\n",
      "0.9999999813735485\n",
      "5\n",
      "0.1999999962747097\n",
      "0.2063008166849613 < 0.1\n",
      "============\n",
      "0.9999999925494194\n",
      "5\n",
      "0.19999999850988387\n",
      "0.20630081444978715 < 0.1\n",
      "============\n",
      "1.0000000223517418\n",
      "5\n",
      "0.20000000447034835\n",
      "0.20630080848932267 < 0.1\n",
      "============\n",
      "0.9999999850988388\n",
      "5\n",
      "0.19999999701976776\n",
      "0.20630081593990326 < 0.1\n",
      "    article_id                                          body_text  category\n",
      "0          229  our live coverage has ended follow the latest ...   protest\n",
      "1          230  fear  greed index   for all the interest in bi...   protest\n",
      "2          231  fear  greed index   fox news reached a lastsec...  positive\n",
      "3          232  cable news network a warner bros discovery com...   protest\n",
      "4          233  millions of americans could face massive conse...      riot\n",
      "5          234  the yearold white man accused of shooting a bl...   protest\n",
      "6          235  cable news network a warner bros discovery com...   protest\n",
      "7          236  its sourdough bread and handstands for jake gy...   protest\n",
      "8          237  a tiny intruder infiltrated white house ground...   protest\n",
      "9          238  jamie foxx remains hospitalized in georgia nea...   protest\n",
      "10         239  a yearold in ohio has died after he took a bun...   protest\n",
      "11         240  cable news network a warner bros discovery com...   protest\n",
      "12         241  fear  greed index   netflix is officially wind...   protest\n",
      "13         242  fear  greed index   artificial intelligence to...   protest\n",
      "14         243  editors note sign up for cnns eat but better m...     other\n",
      "15         375  rohan bopanna is a twotime us open mens double...     other\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def categorize_text(df):\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")\n",
    "\n",
    "    categories = [\"terrorism\", \"natural disaster\",\"positive\",\"protest\",\"riot\"]\n",
    "\n",
    "\n",
    "    threshold = 0.1  # You can adjust this threshold as needed\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['body_text']\n",
    "        result = classifier(text, candidate_labels=categories)\n",
    "        category_scores = {label: score for label, score in zip(result['labels'], result['scores'])}\n",
    "        print(\"=\"*12)\n",
    "        scores_sum = sum(result['scores'])\n",
    "        print(scores_sum)\n",
    "        scores_len = len(result['scores'])\n",
    "        print(scores_len)\n",
    "        scores_avg = scores_sum / scores_len\n",
    "        print(scores_avg)\n",
    "        is_close = all(abs(score - scores_avg) < threshold for score in result['scores'])\n",
    "        print(abs(score - scores_avg),'<',threshold)\n",
    "        if is_close:\n",
    "            top_category = \"other\"\n",
    "        else:\n",
    "            top_category = result['labels'][0]\n",
    "        df.at[index, 'category'] = top_category\n",
    "\n",
    "    return df\n",
    "\n",
    "df_with_categories = categorize_text(df)\n",
    "print(df_with_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"holamigos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celery (Part 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve articles with article Id and have category column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Before i tried going ahead with anything i wanted to ensure that postgres and celery can work together well, This is what i created to see if i can asynchronously, post and retrieve data into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery import Celery\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, DateTime, ARRAY\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "os.environ.setdefault('FORKED_BY_MULTIPROCESSING', '1')\n",
    "\n",
    "app = Celery('tasks', broker='redis://localhost:6379/0')\n",
    "db_string = 'postgresql://kaustubh:concept@localhost:4111/thetimeisnow'\n",
    "engine = create_engine(db_string)\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='celery_errors.log', level=logging.ERROR)\n",
    "\n",
    "metadata = MetaData()\n",
    "\n",
    "articles = Table(\n",
    "    'articles',\n",
    "    metadata,\n",
    "    Column('article_id', Integer, primary_key=True),\n",
    "    Column('title', String),\n",
    "    Column('title_details', String),\n",
    "    Column('published_date', DateTime(timezone=True), default=func.now()),\n",
    "    Column('url', String),\n",
    "    Column('source_id', Integer),\n",
    "    Column('genre_id', ARRAY(String)),\n",
    "    Column('body', String),\n",
    "    Column('header', String)\n",
    ")\n",
    "\n",
    "@app.task\n",
    "def insert_into_articles(data):\n",
    "\n",
    "    try:\n",
    "        pass\n",
    "        with engine.begin() as connection:\n",
    "            connection.execute(articles.insert().values(data))\n",
    "            print(\"executed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error in insertion: {e}')\n",
    "        raise\n",
    "        \n",
    "\n",
    "\n",
    "data = {\n",
    "    'title': 'Sample Title 6',\n",
    "    'title_details': 'Sample Title Details',\n",
    "    'published_date': '2024-01-29 12:00:00' ,\n",
    "    'url': 'http://example.com',\n",
    "    'source_id': 101,\n",
    "    'genre_id': [1,2,3],\n",
    "    'body': 'Sample Body',\n",
    "    'header': 'Sample Header'\n",
    "}\n",
    "#\n",
    "\n",
    "insert_into_articles.delay(data)\n",
    "\n",
    "\n",
    "with open('celery_errors.log', 'r') as log_file:\n",
    "    print(log_file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extracted Fresh data to be able to apply the preprocessing through celery.\n",
    "- Also appended category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    article_id                                          body_text category\n",
      "0          229  our live coverage has ended follow the latest ...         \n",
      "1          230  fear  greed index   for all the interest in bi...         \n",
      "2          231  fear  greed index   fox news reached a lastsec...         \n",
      "3          232  cable news network a warner bros discovery com...         \n",
      "4          233  millions of americans could face massive conse...         \n",
      "5          234  the yearold white man accused of shooting a bl...         \n",
      "6          235  cable news network a warner bros discovery com...         \n",
      "7          236  its sourdough bread and handstands for jake gy...         \n",
      "8          237  a tiny intruder infiltrated white house ground...         \n",
      "9          238  jamie foxx remains hospitalized in georgia nea...         \n",
      "10         239  a yearold in ohio has died after he took a bun...         \n",
      "11         240  cable news network a warner bros discovery com...         \n",
      "12         241  fear  greed index   netflix is officially wind...         \n",
      "13         242  fear  greed index   artificial intelligence to...         \n",
      "14         243  editors note sign up for cnns eat but better m...         \n",
      "15         375  rohan bopanna is a twotime us open mens double...         \n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from sqlalchemy import text as txt\n",
    "\n",
    "db_url = 'postgresql://kaustubh:concept@localhost:4111/thetimeisnow'\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT article_id, body_text\n",
    "    FROM articles;\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(txt(query))\n",
    "    df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df['category'] = ''\n",
    "\n",
    "df = df.head(16)\n",
    "\n",
    "print(df)\n",
    "gdf=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "\n",
    "Here i am creating multiple asynchronous tasks of categorization so they work parallely\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "- Break dataframe into chunks\n",
    "- Run pipeline for each chunk (Divided between celery workers)\n",
    "- Append all parts\n",
    "- Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery import Celery, group\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ.setdefault('FORKED_BY_MULTIPROCESSING', '1')\n",
    "app = Celery('tasks', broker='redis://localhost:6379/0', backend='redis://localhost:6379/1')\n",
    "\n",
    "logging.basicConfig(filename='celery_errors.log', level=logging.ERROR)\n",
    "\n",
    "\n",
    "def chunk_dataframe(df, chunk_size=2):\n",
    "    chunks = [df.iloc[i:i + chunk_size].to_json() for i in range(0, len(df), chunk_size)]\n",
    "    print(\"Dataframe has been chunked\",chunks)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "@app.task\n",
    "def categorize_text_chunk(chunk_json):\n",
    "    chunk = pd.read_json(chunk_json, orient='records')\n",
    "    print(\"inside text chunk right before biggie\",chunk)\n",
    "    return chunk.to_json()\n",
    "    #return categorize_text(chunk)\n",
    "\n",
    "\n",
    "def categorize_text_celery(df):\n",
    "    try:\n",
    "\n",
    "        chunk_jsons = chunk_dataframe(df)\n",
    "\n",
    "        print(\"jsons\")\n",
    "\n",
    "        tasks = [categorize_text_chunk.s(chunk_json) for chunk_json in chunk_jsons]\n",
    "\n",
    "        print(\"tasks was complete\",tasks)\n",
    "        try:\n",
    "            results = group(tasks).apply_async().get(timeout=10)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(\"\\nresults was complete\",results)\n",
    "        reconstructed_chunks = [pd.read_json(result, orient='records') for result in results]\n",
    "\n",
    "        # Concatenate reconstructed chunks into a single DataFrame\n",
    "        categorized_df = pd.concat(reconstructed_chunks, ignore_index=True)\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error in insertion: {e}')\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "df=gdf.copy().head(4)\n",
    "categorized_df = categorize_text_celery(df)\n",
    "\n",
    "print(categorized_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Progress\n",
    "- However this bit here is incomplete and wont run, there is an issue with how celery handles dataframes, since its messaging service is based on json, we cant save a more complex data format in a less complex one.\\\n",
    "\n",
    "- Theres also an issue in how celery gathers results of asynced tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
